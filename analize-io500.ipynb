{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tarfile as trf\n",
    "from os import listdir, system\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(result_folder):\n",
    "    files_tgz = [f for f in listdir(result_folder) if isfile(join(result_folder, f))]\n",
    "\n",
    "    for file in files_tgz:\n",
    "        if file.endswith(\".tgz\"):\n",
    "            tar = trf.open(result_folder + \"/\" + file, \"r:gz\")\n",
    "            tar.extractall(result_folder + \"/\" + \"extract/\")\n",
    "            tar.close()\n",
    "            \n",
    "    result_folder_extracted = result_folder + \"/extract/\"\n",
    "    bench_res_folder = listdir(result_folder_extracted)   \n",
    "    bench_results = []\n",
    "\n",
    "    for fold_name in bench_res_folder:\n",
    "        \n",
    "        with open(result_folder_extracted+fold_name+\"/slurm_script\", \"r\") as file:\n",
    "            data = file.read()\n",
    "            no_of_nodes = (re.findall(r'#SBATCH -N [0-9]*', data)[0]).split()[-1]\n",
    "            no_of_task = (re.findall(r'#SBATCH --ntasks-per-node=[0-9]*', data)[0]).split(\"=\")[-1]\n",
    "        \n",
    "        with open(result_folder_extracted+fold_name+\"/result_summary.txt\", \"r\") as file:  \n",
    "            test_result_data  = \"\" \n",
    "            for line in file.readlines():\n",
    "                \n",
    "                benchmark_result = list(line.split())\n",
    "                if benchmark_result[1]  != \"]\" and len(benchmark_result) > 7:\n",
    "                    test_result_data += (benchmark_result[1]) + \" \" + (benchmark_result[2]) + \" \" + (benchmark_result[3] + \" \") \n",
    "                       \n",
    "            string = str(fold_name + \" \" + no_of_nodes + \" \" + no_of_task + \" \" + result_folder_extracted + \" \" + fold_name + \" \" + test_result_data)\n",
    "            bench_results.append(string.split())\n",
    "            \n",
    "            \n",
    "    system(\"rm -r \" + result_folder + \"/extract\")\n",
    "    df = pd.DataFrame(data=bench_results)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNodesAndTasks(dataFrame):\n",
    "\tnumb_of_nodes = [ int(i) for i in df[1].unique().tolist() ]\n",
    "\tlist.sort(numb_of_nodes)\n",
    "\ttasks = [ int(i) for i in df[2].unique().tolist() ]\n",
    "\tlist.sort(tasks)\n",
    "\ttasks = [str(x) for x in tasks]\n",
    "\treturn (numb_of_nodes, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarkGraphPlot(df, benchmarkDataFrameIndex, mainBenchmarkLabel=\"\", compareToBenchmarkLabel=\"\", pngOut=False ,dfCompareTo=None, benchmarkDataFrameIndexCompareTo=None): \n",
    "\t\n",
    "\tnumb_of_nodes, tasks = getNodesAndTasks(df)\n",
    "\t\n",
    "\tif dfCompareTo is not None:\n",
    "\t\tnumb_of_nodes_cmp, tasks_cmp = getNodesAndTasks(dfCompareTo)\n",
    "\n",
    "\tfig, axs = plt.subplots(len(numb_of_nodes), facecolor=\"white\")\n",
    "\tfig.suptitle(df.iloc[0][benchmarkDataFrameIndex-1])\n",
    "\tfig.set_size_inches(15,20)\n",
    "\tfig.tight_layout(pad=5)\n",
    "\n",
    "\tfor idx, nodes in enumerate(numb_of_nodes):\n",
    "\t\tlegend=[]\n",
    "\t\tsubset_dataframe = df.loc[df[1] == str(nodes)]\n",
    "\t\tdata_list = [ float(x) for x in subset_dataframe[benchmarkDataFrameIndex].to_list()]\n",
    "\t\taxs[idx].plot(tasks, data_list,\"-o\", label=tasks)\n",
    "\t\taxs[idx].set_xticks(labels=tasks, rotation=0, ticks=tasks)\n",
    "\t\taxs[idx].set_xlabel(\"Number of tasks\")\n",
    "\t\taxs[idx].set_ylabel(df.iloc[0][benchmarkDataFrameIndex+1])\n",
    "\t\taxs[idx].set_title(str(nodes) + \" nodes\")\n",
    "\t\tlegend.append(mainBenchmarkLabel)\n",
    "  \n",
    "\t\tif dfCompareTo is not None:\n",
    "\t\t\tsubset_dataframe_cmp = dfCompareTo.loc[dfCompareTo[1] == str(nodes)]\n",
    "\t\t\tdata_list_cmp = [ float(x) for x in subset_dataframe_cmp[benchmarkDataFrameIndexCompareTo].to_list()]\n",
    "\t\t\taxs[idx].plot(tasks_cmp, data_list_cmp,\"-o\", label=tasks)\n",
    "\t\t\taxs[idx].set_ylim(min(min(data_list), min(data_list_cmp))-1, max(max(data_list), max(data_list_cmp))+1)\n",
    "\t\t\tlegend.append(compareToBenchmarkLabel)\n",
    "\t\telse:\n",
    "\t\t\taxs[idx].set_ylim(min(data_list)-0.5, max(data_list)+0.5 )\n",
    "\t\t\t\n",
    "\t\taxs[idx].legend(legend)\n",
    "\tif pngOut:\n",
    "\t\tplt.savefig(str(df.iloc[0][benchmarkDataFrameIndex-1]) + \".png\")\n",
    "\n",
    "\t\t\t\n",
    "\t\n",
    "   \n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getDataset(\"/home/marco/Desktop/io500-optane\") \n",
    "df1 = getDataset(\"/home/marco/Desktop/io500-no-optane\")\n",
    "\n",
    "index = 6\n",
    "while index < len(df.columns):\n",
    "    benchmarkGraphPlot(df, index, pngOut=False , mainBenchmarkLabel=\"optane\", compareToBenchmarkLabel=\"no optane\", dfCompareTo=df1,benchmarkDataFrameIndexCompareTo= index)\n",
    "    index += 3\n",
    "\n",
    "   \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
